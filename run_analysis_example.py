#!/usr/bin/env python3
"""
æ•°æ®é›†åˆ†æè¿è¡Œç¤ºä¾‹
ä½¿ç”¨æ–¹æ³•ï¼š
1. ä¿®æ”¹config_example.pyä¸­çš„é…ç½®å‚æ•°
2. è¿è¡Œæ­¤è„šæœ¬å¼€å§‹åˆ†æ
"""

import os
import sys
from config_example import *
from dataset_analysis_multithreaded import DatasetAnalyzer

def main():
    """
    è¿è¡Œæ•°æ®é›†åˆ†æçš„ä¸»å‡½æ•°
    """
    print("=== æ•°æ®é›†åˆ†ç±»åˆ†æå·¥å…· ===")
    
    # æ£€æŸ¥å¿…è¦çš„é…ç½®
    if not API_KEY:
        print("âŒ é”™è¯¯: è¯·åœ¨config_example.pyä¸­è®¾ç½®API_KEY")
        return False
    
    if not os.path.exists(PARQUET_FILE_PATH):
        print(f"âŒ é”™è¯¯: æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {PARQUET_FILE_PATH}")
        print("è¯·æ£€æŸ¥config_example.pyä¸­çš„PARQUET_FILE_PATHè®¾ç½®")
        return False
    
    print(f"ğŸ“ æ•°æ®é›†æ–‡ä»¶: {PARQUET_FILE_PATH}")
    print(f"ğŸ”§ é…ç½®: {MAX_WORKERS} ä¸ªå¹¶å‘çº¿ç¨‹, æ‰¹æ¬¡å¤§å° {BATCH_SIZE}, æ¨ç†æ¸©åº¦ {TEMPERATURE}, æœ€å¤§è¾“å‡ºé•¿åº¦ {MAX_TOKENS}, å­˜å‚¨ç›®å½• {TEMP_DIR}")
    print(f"ğŸ”„ æœ€å¤§é‡è¯•æ¬¡æ•°: {MAX_RETRIES}")
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­
    confirm = input("\næ˜¯å¦å¼€å§‹åˆ†æ? (y/N): ").strip().lower()
    if confirm not in ['y', 'yes']:
        print("å·²å–æ¶ˆåˆ†æ")
        return False
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = DatasetAnalyzer(
        api_key=API_KEY,
        max_workers=MAX_WORKERS,
        max_retries=MAX_RETRIES,
        temperature=TEMPERATURE,
        max_len=MAX_TOKENS,
        output=TEMP_DIR,
        enable_mislabel_analysis=ENABLE_MISLABEL_ANALYSIS,
        enable_article_summary=ENABLE_ARTICLE_SUMMARY
    )
    
    try:
        print("\nğŸš€ å¼€å§‹åˆ†æ...")
        
        # å¼€å§‹åˆ†æ
        results = analyzer.analyze_dataset(
            parquet_file=PARQUET_FILE_PATH,
            batch_size=BATCH_SIZE,
            start_from_batch=START_FROM_BATCH
        )
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if 'error' in results:
            print(f"âŒ åˆ†æå¤±è´¥: {results['error']}")
            return False
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print("\nâœ… åˆ†æå®Œæˆ!")
        print(f"ğŸ“Š æ€»è®¡å¤„ç†: {results['metadata']['total_processed']} é¡¹")
        print(f"âŒ å¤±è´¥é¡¹ç›®: {results['metadata']['total_failed']} é¡¹")
        
        if results['metadata']['total_processed'] > 0:
            success_rate = (results['metadata']['total_processed'] / 
                          (results['metadata']['total_processed'] + results['metadata']['total_failed'])) * 100
            print(f"âœ… æˆåŠŸç‡: {success_rate:.1f}%")
            
            # æ˜¾ç¤ºåˆ†ç±»åˆ†å¸ƒ
            summary = results['summary']
            print("\nğŸ“ˆ åˆ†ç±»åˆ†å¸ƒ:")
            for class_type, count in summary['classification_distribution'].items():
                percentage = (count / results['metadata']['total_processed']) * 100
                print(f"  {class_type}: {count} ({percentage:.1f}%)")
            
            # æ˜¾ç¤ºå‡†ç¡®æ€§åˆ†æï¼ˆå½“å¯ç”¨è¯¯æ ‡æ³¨åˆ†ææ—¶ï¼‰
            if 'accuracy_analysis' in summary:
                accuracy = summary['accuracy_analysis']
                if accuracy['total_analyzed'] > 0:
                    correct_rate = (accuracy['correct_classifications'] / accuracy['total_analyzed']) * 100
                    print(f"\nğŸ¯ åˆ†ç±»å‡†ç¡®æ€§: {correct_rate:.1f}% ({accuracy['correct_classifications']}/{accuracy['total_analyzed']})")
            
            # æ˜¾ç¤ºé«˜é¢‘å…³é”®è¯
            print("\nğŸ” é«˜é¢‘å…³é”®è¯ (å‰10ä¸ª):")
            for i, (keyword, freq) in enumerate(list(summary['top_keywords'].items())[:10], 1):
                print(f"  {i}. {keyword}: {freq}")
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {FINAL_RESULTS_FILE}")
        print(f"ğŸ“ ä¸­é—´ç»“æœç›®å½•: {TEMP_DIR}")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")
        
        # å±•ç¤ºéƒ¨åˆ†æ–‡ç« çº§åˆ«è§„å¾‹
        if ENABLE_ARTICLE_SUMMARY and results.get('article_summaries'):
            print("\nğŸ§© æ–‡ç« çº§åˆ«æç‚¼è§„å¾‹(ç¤ºä¾‹å‰3ç¯‡):")
            shown = 0
            for aid, summary_item in results['article_summaries'].items():
                print(f"\nğŸ“„ Article {aid}")
                para = summary_item.get('refined_rules_paragraph', '') or ''
                print(f"  æ‘˜è¦: {para[:300]}{'...' if len(para) > 300 else ''}")
                shown += 1
                if shown >= 3:
                    break
        
        return True
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­äº†åˆ†æè¿‡ç¨‹")
        print(f"ğŸ’¡ æç¤º: å¯ä»¥è®¾ç½®START_FROM_BATCHå‚æ•°ä»ä¸­æ–­å¤„æ¢å¤")
        return False
        
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return False

def resume_analysis():
    """
    æ¢å¤ä¸­æ–­çš„åˆ†æä»»åŠ¡
    """
    checkpoint_file = os.path.join(TEMP_DIR, "checkpoint.json")
    
    if not os.path.exists(checkpoint_file):
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œæ— æ³•æ¢å¤")
        return False
    
    try:
        import json
        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            checkpoint = json.load(f)
        
        last_batch = checkpoint.get('last_completed_batch', -1)
        total_completed = checkpoint.get('total_completed', 0)
        
        print(f"ğŸ”„ å‘ç°æ£€æŸ¥ç‚¹:")
        print(f"  ä¸Šæ¬¡å®Œæˆæ‰¹æ¬¡: {last_batch}")
        print(f"  å·²å®Œæˆé¡¹ç›®: {total_completed}")
        print(f"  æ—¶é—´æˆ³: {checkpoint.get('timestamp', 'unknown')}")
        
        confirm = input(f"\næ˜¯å¦ä»æ‰¹æ¬¡ {last_batch + 1} å¼€å§‹æ¢å¤? (y/N): ").strip().lower()
        if confirm in ['y', 'yes']:
            # æ›´æ–°é…ç½®å¹¶è¿è¡Œ
            global START_FROM_BATCH
            START_FROM_BATCH = last_batch + 1
            return main()
        else:
            print("å·²å–æ¶ˆæ¢å¤")
            return False
            
    except Exception as e:
        print(f"âŒ è¯»å–æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--resume":
        # æ¢å¤æ¨¡å¼
        resume_analysis()
    else:
        # æ”¯æŒç‹¬ç«‹çš„æ–‡ç« çº§å½’çº³
        if len(sys.argv) > 1 and sys.argv[1] == "--summarize-articles":
            analyzer = DatasetAnalyzer(
                api_key=API_KEY or "dummy",  # æ–‡ç« çº§æ±‡æ€»å¯ä¸éœ€è¦çœŸå®è°ƒç”¨ï¼Œå¦‚éœ€LLMé‡æç‚¼ä»å¯ç”¨
                max_workers=MAX_WORKERS,
                max_retries=MAX_RETRIES,
                temperature=TEMPERATURE,
                max_len=MAX_TOKENS,
                output=TEMP_DIR,
                enable_mislabel_analysis=ENABLE_MISLABEL_ANALYSIS,
                enable_article_summary=False
            )
            try:
                from config_example import ARTICLE_SUMMARY_INPUT_FILE, ARTICLE_SUMMARY_OUTPUT_FILE
            except Exception:
                ARTICLE_SUMMARY_INPUT_FILE = FINAL_RESULTS_FILE
                ARTICLE_SUMMARY_OUTPUT_FILE = "article_summaries.json"

            print("\nğŸ§© åŸºäºå·²å­˜åœ¨ç»“æœè¿›è¡Œæ–‡ç« çº§å½’çº³...")
            summaries = analyzer.summarize_articles_from_file(
                input_file=ARTICLE_SUMMARY_INPUT_FILE,
                output_file=ARTICLE_SUMMARY_OUTPUT_FILE
            )
            if 'error' in summaries:
                print(f"âŒ æ–‡ç« çº§å½’çº³å¤±è´¥: {summaries['error']}")
                sys.exit(1)
            print(f"âœ… å®Œæˆã€‚è¾“å‡ºä½äº {TEMP_DIR}/{ARTICLE_SUMMARY_OUTPUT_FILE}")
        else:
            # æ­£å¸¸æ¨¡å¼
            main()
