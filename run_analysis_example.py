#!/usr/bin/env python3
"""
Êï∞ÊçÆÈõÜÂàÜÊûêËøêË°åÁ§∫‰æã
‰ΩøÁî®ÊñπÊ≥ïÔºö
1. ‰øÆÊîπconfig_example.py‰∏≠ÁöÑÈÖçÁΩÆÂèÇÊï∞
2. ËøêË°åÊ≠§ËÑöÊú¨ÂºÄÂßãÂàÜÊûê
"""

import os
import sys
from config_example import *
from dataset_analysis_multithreaded import DatasetAnalyzer

def main():
    """
    ËøêË°åÊï∞ÊçÆÈõÜÂàÜÊûêÁöÑ‰∏ªÂáΩÊï∞
    """
    print("=== Êï∞ÊçÆÈõÜÂàÜÁ±ªÂàÜÊûêÂ∑•ÂÖ∑ ===")
    
    # Ê£ÄÊü•ÂøÖË¶ÅÁöÑÈÖçÁΩÆ
    if not API_KEY:
        print("‚ùå ÈîôËØØ: ËØ∑Âú®config_example.py‰∏≠ËÆæÁΩÆAPI_KEY")
        return False
    
    if not os.path.exists(PARQUET_FILE_PATH):
        print(f"‚ùå ÈîôËØØ: Êï∞ÊçÆÈõÜÊñá‰ª∂‰∏çÂ≠òÂú®: {PARQUET_FILE_PATH}")
        print("ËØ∑Ê£ÄÊü•config_example.py‰∏≠ÁöÑPARQUET_FILE_PATHËÆæÁΩÆ")
        return False
    
    print(f"üìÅ Êï∞ÊçÆÈõÜÊñá‰ª∂: {PARQUET_FILE_PATH}")
    print(f"üîß ÈÖçÁΩÆ: {MAX_WORKERS} ‰∏™Âπ∂ÂèëÁ∫øÁ®ã, ÊâπÊ¨°Â§ßÂ∞è {BATCH_SIZE}, Êé®ÁêÜÊ∏©Â∫¶ {TEMPERATURE}, ÊúÄÂ§ßËæìÂá∫ÈïøÂ∫¶ {MAX_TOKENS}, Â≠òÂÇ®ÁõÆÂΩï {TEMP_DIR}")
    print(f"üîÑ ÊúÄÂ§ßÈáçËØïÊ¨°Êï∞: {MAX_RETRIES}")
    
    # ËØ¢ÈóÆÊòØÂê¶ÁªßÁª≠
    confirm = input("\nÊòØÂê¶ÂºÄÂßãÂàÜÊûê? (y/N): ").strip().lower()
    if confirm not in ['y', 'yes']:
        print("Â∑≤ÂèñÊ∂àÂàÜÊûê")
        return False
    
    # ÂàõÂª∫ÂàÜÊûêÂô®ÂÆû‰æã
    analyzer = DatasetAnalyzer(
        api_key=API_KEY,
        max_workers=MAX_WORKERS,
        max_retries=MAX_RETRIES,
        temperature=TEMPERATURE,
        max_len=MAX_TOKENS,
        output=TEMP_DIR,
        enable_mislabel_analysis=ENABLE_MISLABEL_ANALYSIS
    )
    
    try:
        print("\nüöÄ ÂºÄÂßãÂàÜÊûê...")
        
        # ÂºÄÂßãÂàÜÊûê
        results = analyzer.analyze_dataset(
            parquet_file=PARQUET_FILE_PATH,
            batch_size=BATCH_SIZE,
            start_from_batch=START_FROM_BATCH
        )
        
        # Ê£ÄÊü•ÊòØÂê¶ÊúâÈîôËØØ
        if 'error' in results:
            print(f"‚ùå ÂàÜÊûêÂ§±Ë¥•: {results['error']}")
            return False
        
        # ÊâìÂç∞ÁªìÊûúÊëòË¶Å
        print("\n‚úÖ ÂàÜÊûêÂÆåÊàê!")
        print(f"üìä ÊÄªËÆ°Â§ÑÁêÜ: {results['metadata']['total_processed']} È°π")
        print(f"‚ùå Â§±Ë¥•È°πÁõÆ: {results['metadata']['total_failed']} È°π")
        
        if results['metadata']['total_processed'] > 0:
            success_rate = (results['metadata']['total_processed'] / 
                          (results['metadata']['total_processed'] + results['metadata']['total_failed'])) * 100
            print(f"‚úÖ ÊàêÂäüÁéá: {success_rate:.1f}%")
            
            # ÊòæÁ§∫ÂàÜÁ±ªÂàÜÂ∏É
            summary = results['summary']
            print("\nüìà ÂàÜÁ±ªÂàÜÂ∏É:")
            for class_type, count in summary['classification_distribution'].items():
                percentage = (count / results['metadata']['total_processed']) * 100
                print(f"  {class_type}: {count} ({percentage:.1f}%)")
            
            # ÊòæÁ§∫ÂáÜÁ°ÆÊÄßÂàÜÊûêÔºàÂΩìÂêØÁî®ËØØÊ†áÊ≥®ÂàÜÊûêÊó∂Ôºâ
            if 'accuracy_analysis' in summary:
                accuracy = summary['accuracy_analysis']
                if accuracy['total_analyzed'] > 0:
                    correct_rate = (accuracy['correct_classifications'] / accuracy['total_analyzed']) * 100
                    print(f"\nüéØ ÂàÜÁ±ªÂáÜÁ°ÆÊÄß: {correct_rate:.1f}% ({accuracy['correct_classifications']}/{accuracy['total_analyzed']})")
            
            # ÊòæÁ§∫È´òÈ¢ëÂÖ≥ÈîÆËØç
            print("\nüîç È´òÈ¢ëÂÖ≥ÈîÆËØç (Ââç10‰∏™):")
            for i, (keyword, freq) in enumerate(list(summary['top_keywords'].items())[:10], 1):
                print(f"  {i}. {keyword}: {freq}")
        
        print(f"\nüíæ ËØ¶ÁªÜÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞: {FINAL_RESULTS_FILE}")
        print(f"üìÅ ‰∏≠Èó¥ÁªìÊûúÁõÆÂΩï: {TEMP_DIR}")
        print(f"üìù Êó•ÂøóÊñá‰ª∂: {LOG_FILE}")
        
        return True
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Áî®Êà∑‰∏≠Êñ≠‰∫ÜÂàÜÊûêËøáÁ®ã")
        print(f"üí° ÊèêÁ§∫: ÂèØ‰ª•ËÆæÁΩÆSTART_FROM_BATCHÂèÇÊï∞‰ªé‰∏≠Êñ≠Â§ÑÊÅ¢Â§ç")
        return False
        
    except Exception as e:
        print(f"\n‚ùå ÂàÜÊûêËøáÁ®ã‰∏≠Âá∫Áé∞ÈîôËØØ: {e}")
        return False

def resume_analysis():
    """
    ÊÅ¢Â§ç‰∏≠Êñ≠ÁöÑÂàÜÊûê‰ªªÂä°
    """
    checkpoint_file = os.path.join(TEMP_DIR, "checkpoint.json")
    
    if not os.path.exists(checkpoint_file):
        print("‚ùå Ê≤°ÊúâÊâæÂà∞Ê£ÄÊü•ÁÇπÊñá‰ª∂ÔºåÊó†Ê≥ïÊÅ¢Â§ç")
        return False
    
    try:
        import json
        with open(checkpoint_file, 'r', encoding='utf-8') as f:
            checkpoint = json.load(f)
        
        last_batch = checkpoint.get('last_completed_batch', -1)
        total_completed = checkpoint.get('total_completed', 0)
        
        print(f"üîÑ ÂèëÁé∞Ê£ÄÊü•ÁÇπ:")
        print(f"  ‰∏äÊ¨°ÂÆåÊàêÊâπÊ¨°: {last_batch}")
        print(f"  Â∑≤ÂÆåÊàêÈ°πÁõÆ: {total_completed}")
        print(f"  Êó∂Èó¥Êà≥: {checkpoint.get('timestamp', 'unknown')}")
        
        confirm = input(f"\nÊòØÂê¶‰ªéÊâπÊ¨° {last_batch + 1} ÂºÄÂßãÊÅ¢Â§ç? (y/N): ").strip().lower()
        if confirm in ['y', 'yes']:
            # Êõ¥Êñ∞ÈÖçÁΩÆÂπ∂ËøêË°å
            global START_FROM_BATCH
            START_FROM_BATCH = last_batch + 1
            return main()
        else:
            print("Â∑≤ÂèñÊ∂àÊÅ¢Â§ç")
            return False
            
    except Exception as e:
        print(f"‚ùå ËØªÂèñÊ£ÄÊü•ÁÇπÂ§±Ë¥•: {e}")
        return False

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--resume":
        # ÊÅ¢Â§çÊ®°Âºè
        resume_analysis()
    else:
        # Ê≠£Â∏∏Ê®°Âºè
        main()
